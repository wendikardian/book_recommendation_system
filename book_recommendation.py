# -*- coding: utf-8 -*-
"""Book_recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1keA8qCaZ-BmicL1Cj3NMtqi5kHN7qfZZ
"""

!pip install kaggle

"""# Download dan siapkan dataset"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

!kaggle datasets download -d arashnic/book-recommendation-dataset

!mkdir data
!unzip book-recommendation-dataset.zip -d data
!ls data

"""# Import library yang dibutuhkan dan check isi data"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

books = pd.read_csv('data/Books.csv')
ratings = pd.read_csv('data/Ratings.csv')
users = pd.read_csv('data/Users.csv')

books

ratings

users

print(books.columns)
print(users.columns)
print(ratings.columns)

"""# Data Understanding

Code ratings.describe() digunakan untuk menghasilkan ringkasan statistik deskriptif dari dataset ratings. Fungsi ini memberikan gambaran cepat tentang distribusi statistik dari kolom-kolom numerik dalam dataset.
"""

ratings.describe()

books.describe()

"""## Check Missing Value dari dataset"""

print("Missing value form books ", books.isnull().sum())
print("Missing value form users ", users.isnull().sum())
print("Missing value form ratings ", ratings.isnull().sum())

"""Hapus column 'age' karena tidak dibutuhkan dalam data processing"""

users = users.drop(columns=['Age'])


print(users.head())

"""## Check duplicates row dari masing-masing dataset"""

books_duplicates_count = books.duplicated().sum()

print("Duplicated rows count in 'books' DataFrame:", books_duplicates_count)

users_duplicates_count = users.duplicated().sum()
print("Duplicated rows count in 'users' DataFrame:", users_duplicates_count)

ratings_duplicates_count = ratings.duplicated().sum()
print("Duplicated rows count in 'ratings' DataFrame:", ratings_duplicates_count)

"""Check kembali dataset setelah rows yang duplicates dihapus dalam dataset"""

import pandas as pd

total_books = books.shape[0]
total_users = users.shape[0]
total_ratings = ratings.shape[0]

print(f"Total number of rows in the 'Books' dataset: {total_books}")
print(f"Total number of rows in the 'Users' dataset: {total_users}")
print(f"Total number of rows in the 'Ratings' dataset: {total_ratings}")

"""## Check unique value dari dataset books dan ratings berdasarkan ISBN dan id user"""

import pandas as pd

unique_isbns_ratings = ratings['ISBN'].nunique()

print(f"Total number of unique ISBNs in the 'Ratings' dataset: {unique_isbns_ratings}")

import pandas as pd


unique_user_ids_ratings = ratings['User-ID'].nunique()

print(f"Total number of unique UserIDs in the 'Ratings' dataset: {unique_user_ids_ratings}")

"""## Data Visualisasi
Visualisasi disribusi ratings dari dataset Ratings
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
sns.countplot(x='Book-Rating', data=ratings)
plt.title('Distribution of Book Ratings in Ratings Dataset')
plt.xlabel('Book Rating')
plt.ylabel('Count')
plt.show()

"""### Top 10 Publishers with Most Books Published"""

top_publishers = books['Publisher'].value_counts().head(10)


plt.figure(figsize=(12, 6))
sns.barplot(x=top_publishers.values, y=top_publishers.index, palette='viridis')
plt.title('Top 10 Publishers with Most Books Published')
plt.xlabel('Number of Books Published')
plt.ylabel('Publisher')
plt.show()

"""### Top 10 Authors with Most Books Published"""

top_authors = books['Book-Author'].value_counts().head(10)


plt.figure(figsize=(12, 6))
sns.barplot(x=top_authors.values, y=top_authors.index, palette='mako')
plt.title('Top 10 Authors with Most Books Published')
plt.xlabel('Number of Books Published')
plt.ylabel('Author')
plt.show()

"""### Top 10 Users with Most Ratings"""

top_rated_users = ratings['User-ID'].value_counts().head(10)

plt.figure(figsize=(12, 6))
sns.barplot(x=top_rated_users.index, y=top_rated_users.values, palette='Blues')
plt.title('Top 10 Users with Most Ratings')
plt.xlabel('User ID')
plt.ylabel('Number of Ratings')
plt.show()

"""### Top 10 Books with Highest Average Ratings"""

average_ratings = ratings.groupby('ISBN')['Book-Rating'].mean()

books_with_avg_ratings = pd.merge(books, average_ratings, left_on='ISBN', right_index=True, how='left')
books_with_avg_ratings.rename(columns={'Book-Rating': 'Average-Rating'}, inplace=True)

top_rated_books = books_with_avg_ratings.sort_values(by='Average-Rating', ascending=False).head(10)

plt.figure(figsize=(12, 6))
sns.barplot(x='Average-Rating', y='Book-Title', data=top_rated_books, palette='viridis')
plt.title('Top 10 Books with Highest Average Ratings')
plt.xlabel('Average Rating')
plt.ylabel('Book Title')
plt.show()

"""### Top 10 Books with Most Ratings"""

ratings_count = ratings['ISBN'].value_counts()

books_with_ratings_count = pd.merge(books, ratings_count, left_on='ISBN', right_index=True, how='left')
books_with_ratings_count.rename(columns={'ISBN_y': 'Ratings-Count'}, inplace=True)

top_rated_books = books_with_ratings_count.sort_values(by='Ratings-Count', ascending=False).head(10)

plt.figure(figsize=(12, 6))
sns.barplot(x='Ratings-Count', y='Book-Title', data=top_rated_books, palette='magma')
plt.title('Top 10 Books with Most Ratings')
plt.xlabel('Number of Ratings')
plt.ylabel('Book Title')
plt.show()

"""#### Check data unique berdasarkan ISBN dan User-ID

menggabungkan dan menghitung jumlah unik User-ID dan ISBN yang muncul dalam dataset users, books, dan ratings, serta menampilkan jumlah total unik User-IDs dan ISBNs yang terdapat di seluruh dataset tersebut.
"""

unique_user_ids = np.concatenate((
    users['User-ID'].unique(),
    ratings['User-ID'].unique()
))

unique_isbns = np.concatenate((
    books['ISBN'].unique(),
    ratings['ISBN'].unique()
))

unique_user_ids = np.sort(np.unique(unique_user_ids))
unique_isbns = np.sort(np.unique(unique_isbns))

print('Number of unique User-IDs across datasets: ', len(unique_user_ids))
print('Number of unique ISBNs across datasets: ', len(unique_isbns))

unique_isbns

unique_user_ids

"""penggabungan (merge) antara dataset ratings dan books berdasarkan kolom ISBN, dan menampilkan dimensi (shape) dari kedua dataset sebelum dan setelah penggabungan."""

print("before merged")
print("ratings shape:",ratings.shape)
print("Books shape:",books.shape)
books_ratings_collab=ratings.merge(books,on='ISBN')
print("after merged:",books_ratings_collab.shape)

"""### Hasil penggabungan dataset"""

books_ratings_collab

books_ratings_collab.info()

books_ratings_collab.describe()

"""### Group data berdasarkan ISBN dan User-ID"""

grouped_data = books_ratings_collab.groupby(['ISBN', 'User-ID']).agg({'Book-Rating': 'mean'}).reset_index()
grouped_data

books_ratings_collab.columns

books_ratings_collab.isnull().sum()

books_ratings_clean = books_ratings_collab.dropna()
books_ratings_clean

books_ratings_clean.isnull().sum()

books_fix = books_ratings_clean.sort_values('ISBN', ascending = True)
books_fix

len(books_fix.ISBN.unique())

books_fix.ISBN.unique()

len(books_fix['Book-Author'].unique())

preparation = books_fix.drop_duplicates('ISBN')
preparation

book_ISBN = preparation['ISBN'].tolist()
book_title = preparation['Book-Title'].tolist()
book_author = preparation['Book-Author'].tolist()
book_publisher = preparation['Publisher'].tolist()

print(len(book_ISBN))
print(len(book_title))
print(len(book_author))
print(len(book_publisher))

"""# Content Based Filtering

#### Menyiapkan dataset baru sebelum nanti masuk ke tahap Content Based Filtering

membuat DataFrame baru (book_new) yang berisi kolom-kolom 'isbn', 'title', 'author', dan 'publisher' dari dataset buku. DataFrame ini kemudian diurutkan berdasarkan kolom 'author' secara ascending (menaik) dan hanya menyertakan 50,000 baris pertama.
"""

book_new = pd.DataFrame({
    'isbn' : book_ISBN,
    'title': book_title,
    'author' : book_author,
    'publisher' : book_publisher
})

book_new = book_new.sort_values('author', ascending = True)
book_new = book_new.head(50000)
book_new

data = book_new
data.sample(5)

"""### Vectorization menggunakan TFIDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()
tf.fit(data['author'])

tf.get_feature_names_out()

""" metode TF-IDF (Term Frequency-Inverse Document Frequency) untuk mengubah teks di dalam kolom 'author' dari dataset 'data' menjadi representasi matriks TF-IDF, dan menampilkan dimensi (shape) dari matriks tersebut."""

tfidf_matrix = tf.fit_transform(data['author'])

tfidf_matrix.shape

tfidf_matrix.todense()

"""mengonversi matriks TF-IDF menjadi DataFrame Pandas dengan menggunakan representasi dense, kemudian menampilkan contoh 22 kolom acak dan 10 baris acak dari DataFrame tersebut."""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns = tf.get_feature_names_out(),
    index = data.title,
).sample(22, axis = 1).sample(10, axis = 0)

"""### Cosine Similarity

menggunakan cosine similarity untuk menghitung matriks similarity antar data berdasarkan representasi TF-IDF, menyajikan nilai-nilai similarity kosinus antar data dalam dataset.
"""

from sklearn.metrics.pairwise import cosine_similarity


cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""DataFrame (cosine_sim_df) berukuran sesuai dengan jumlah judul buku dalam dataset, yang berisi nilai similarity kosinus antar judul buku berdasarkan matriks kesamaan kosinus sebelumnya"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns = data['title'])
print('Shape : ', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis = 1).sample(10, axis = 0)

"""### Buat Rekomendasi


Function book_recommendations digunakan untuk memberikan rekomendasi buku berdasarkan kemiripan kesamaan kosinus dengan buku yang diberikan, menggunakan matriks kesamaan kosinus (cosine_sim_df) dan dataset informasi buku (data), dengan jumlah rekomendasi yang dapat disesuaikan (k).
"""

def book_recommendations(title, similarity_data=cosine_sim_df, items=data[['title', 'author']], k=5):
    """
    Rekomendasi Resto berdasarkan kemiripan dataframe

    Parameter:
    ---
    title : tipe data string (str)
                Nama Buku (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan buku sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """

    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    closest = closest.drop(title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

data.sample(2)

"""### Contoh Hasil Rekomendasi"""

book_recommendations("Delta Blood")

ground_truth = {
    '51655': ['Homeward Winds the River', "Tara's Song", 'The Heirs of Love', 'Christian Acts of Kindness', 'Living Somewhere Between Estrogen and Death	']
}

precision_values = []
recall_values = []

for user_id, actual_books in ground_truth.items():
    recommendations = book_recommendations('Delta Blood')['title'].tolist()
    tp = len(set(actual_books) & set(recommendations))
    fp = len(set(recommendations) - set(actual_books))
    fn = len(set(actual_books) - set(recommendations))

    precision = tp / (tp + fp) if (tp + fp) != 0 else 0
    recall = tp / (tp + fn) if (tp + fn) != 0 else 0

    precision_values.append(precision)
    recall_values.append(recall)

average_precision = sum(precision_values) / len(precision_values)
average_recall = sum(recall_values) / len(recall_values)

print(f"Average Precision: {average_precision}")
print(f"Average Recall: {average_recall}")

"""# Collaborative Filtering"""

import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

df = ratings
df = df.head(50000)
df

"""### Encoding User

membuat pemetaan unik antara User-ID dalam dataset ke representasi numerik, baik dari User-ID ke indeks (encoded) maupun sebaliknya, menggunakan dictionary dalam Python.

"""

user_ids = df['User-ID'].unique().tolist()
print('list userID: ', user_ids)

user_to_user_encoded = {
    x: i for i, x in enumerate(user_ids)
}
# print('encoded userID : ', user_to_user_encoded)

user_encoded_to_user = {
    i : x for i, x in enumerate(user_ids)
}
user_to_user_encoded
# print('encoded angka ke userID : ', user_encoded_to_user)

"""### Encoding Books

membuat pemetaan unik antara ISBN buku dalam dataset ke representasi numerik, baik dari ISBN ke indeks (encoded) maupun sebaliknya, menggunakan dictionary dalam Python.
"""

book_isbn = df['ISBN'].unique().tolist()

book_to_book_encoded = {
    x: i for i, x in enumerate(book_isbn)
}

book_encoded_to_book = {
    i: x for i, x in enumerate(book_isbn)
}

book_encoded_to_book

"""### Mapping Data

Menambahkan dua kolom baru, 'user' dan 'book', ke dalam DataFrame 'df', yang merupakan representasi numerik dari User-ID dan ISBN berdasarkan pemetaan yang telah dibuat sebelumnya (user_to_user_encoded dan book_to_book_encoded).
"""

df['user'] = df['User-ID'].map(user_to_user_encoded)

df['book'] = df['ISBN'].map(book_to_book_encoded)

df

num_users = len(user_to_user_encoded)
print(num_users)

num_book = len(book_encoded_to_book)
print(num_book)

df['rating'] = df['Book-Rating'].values.astype(np.float32)
min_rating = min(df['rating'])
max_rating = max(df['rating'])
print('Number of user : {}, Number of book : {}, Min Rating : {}, Max Rating : {}'. format(num_users, num_book, min_rating, max_rating))

df = df.sample(frac = 1, random_state = 42)
df

"""### Split the Data

mempersiapkan data untuk pelatihan model rekomendasi, dengan mengubah nilai rating ke dalam rentang [0, 1] menggunakan normalisasi dan membagi dataset menjadi data pelatihan (x_train, y_train) dan data validasi (x_val, y_val).
"""

x = df[['user', 'book']].values


y = df['rating'].apply(lambda x : (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)


print(x, y)

"""### Neural Network Model untuk Sistem Rekomendasi

menggunakan lapisan embedding untuk merepresentasikan pengguna (user) dan buku (book), serta menerapkan function panggilan (call) untuk menghasilkan prediksi rekomendasi berdasarkan interaksi antara pengguna dan buku.
"""

class RecommenderNet(tf.keras.Model):
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.book_embedding = layers.Embedding(
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:, 0])
    user_bias = self.user_bias(inputs[:, 0])
    book_vector = self.book_embedding(inputs[:, 1])
    book_bias = self.book_bias(inputs[:, 1])

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_book, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate = 0.001),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

"""### Train the Model"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""### Model Evaluation

menampilkan perbandingan nilai root mean squared error (RMSE) antara data pelatihan dan data validasi pada setiap epoch selama pelatihan model.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

book_new

"""## Menguji Model Untuk Sistem Rekomendasi"""

book_df = book_new

data = pd.read_csv('data/Ratings.csv')
user_id = df['User-ID'].sample(1).iloc[0]
book_read_by_user = data[data['User-ID'] == user_id]


book_not_rated = book_df[~book_df['isbn'].isin(book_read_by_user.ISBN.values)]['isbn']
book_not_rated = list(
    set(book_not_rated).intersection(set(book_to_book_encoded.keys()))
)
book_not_rated = [[book_to_book_encoded.get(x)] for x in book_not_rated ]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_rated), book_not_rated)
)
user_book_array

book_df

"""## Contoh Hasil Rekomendasi"""

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_rated[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users : {}'.format(user_id))
print('===' * 9)
print('book with high ratings from user')
print('----' * 8)

top_book_user = (
    book_read_by_user.sort_values(by='Book-Rating', ascending=False)
).head(5).ISBN.values

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]
for row in book_df_rows.itertuples():
  print(row.title, ':', row.author)


print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)

recommended_book = book_df[book_df['isbn'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
  print(row.title, ':', row.author)

from sklearn.metrics import precision_score, recall_score, f1_score

def get_model_predictions(model, x):
    return model.predict(x).round().flatten()

x_test = x_val
predictions = get_model_predictions(model, x_test)
ground_truth = y_val

tp = sum((predictions == 1) & (ground_truth == 1))
fp = sum((predictions == 1) & (ground_truth == 0))
fn = sum((predictions == 0) & (ground_truth == 1))

precision = tp / (tp + fp) if (tp + fp) != 0 else 0
recall = tp / (tp + fn) if (tp + fn) != 0 else 0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")